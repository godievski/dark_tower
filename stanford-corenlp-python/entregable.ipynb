{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import os\n",
    "from os.path import isfile, join, isdir\n",
    "from itertools import compress\n",
    "import numpy as np\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listando directorios de libros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de libros: 7\n"
     ]
    }
   ],
   "source": [
    "dir_path = 'data'\n",
    "#os.listdir(dir_path)\n",
    "listdir = [dir_path + '/' + d for d in os.listdir(dir_path) if isdir(join(dir_path, d))]\n",
    "listdir.sort()\n",
    "\n",
    "print ('Cantidad de libros: ' + str(len(listdir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_tag = ['p', 'p', 'div', 'p', 'p', 'p', 'p' ]\n",
    "list_class = [['para-center', 'para-flush','para-indent', 'para-quote', 'para-ragged-left', 'para-ragged-right', 'sans-para-center', 'sans-para-flush', 'sans-para-indent', ''],\n",
    "              ['para-flush','para-indent'],\n",
    "              ['tx','tx1'], \n",
    "              ['begin', 'para-indent', 'para-flush','para-ragged-left' ],\n",
    "              ['block','blockb','blocki','indent','indent1','indent1b','indenta', 'indentab','indentb','indenti', 'noindent','noindenta','noindentb'],\n",
    "              ['block','blocki','blockt','blockti','indent', 'indenta', 'indentb', 'noindent', 'noindent1', 'noindenta', 'noindentb', 'poem', 'poem1', 'poema', 'right', 'right1'],\n",
    "              ['indent', 'indentb', 'indentb1', 'noindent', 'noindentc', 'noindentn', 'noindentn1']\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obteniendo el texto de cada libro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validar_parrafo_no_tag(parrafo):\n",
    "    tags = ['<', '>']\n",
    "    result = [tag in parrafo for tag in tags]\n",
    "    final = any(result)\n",
    "    return not final\n",
    "\n",
    "def print_text_if_class(parrafo, html_cl):\n",
    "    if(html_cl in parrafo['class']):\n",
    "        print(''.join(parrafo.findAll(text=True)))\n",
    "\n",
    "def validar_parrafo_clases(parrafo, clases):\n",
    "    l_valido = [c in parrafo['class'] for c in clases]\n",
    "    return any(l_valido)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leer_parrafos(soupObj, html_tag, html_clases):\n",
    "    parrafos = soupObj.find_all(html_tag)\n",
    "    parrafos_validos = [validar_parrafo_clases(p, html_clases) for p in parrafos]\n",
    "    parrafos_final = list(compress(parrafos, parrafos_validos))\n",
    "    parrafos_str = [unicode(''.join(p.findAll(text=True))) \\\n",
    "                    for p in parrafos_final \\\n",
    "                    if type(p.findAll(text=True)[0]) == bs4.element.NavigableString]\n",
    "    return parrafos_str\n",
    "\n",
    "def leer_parrafos_bs4(soupObj, html_tag):\n",
    "    parrafos = soupObj.find_all(html_tag)\n",
    "    return parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leer_libro(book_path, html_tag, html_clases):\n",
    "    list_files = [book_path + '/' + f for f in os.listdir(book_path) if isfile(join(book_path, f))]\n",
    "    list_files.sort()\n",
    "    print ('Cantidad de archivos en <' + book_path + '>: ' + str(len(list_files)))\n",
    "    \n",
    "    list_html = [open(f,'r') for f in list_files]\n",
    "    l_BsOj = [ BeautifulSoup(html, 'html5lib') for html in list_html]\n",
    "    \n",
    "    parrafos_matriz = [leer_parrafos(soupObj, html_tag, html_clases) for soupObj in l_BsOj]\n",
    "    parrafos_array = [unidecode(p) for p_list in parrafos_matriz for p in p_list]\n",
    "    \n",
    "    l_vf = [validar_parrafo_no_tag(p) for p in parrafos_array]\n",
    "    verificar = all(l_vf)\n",
    "    total = len(parrafos_array)\n",
    "    \n",
    "    print('Parrafos validos? ' + str(verificar))\n",
    "    print('Cantidad total de parrafos: ' + str(total))\n",
    "    return parrafos_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag html a analizar: <p>\n",
      "Clases html a analizar: para-center, para-flush, para-indent, para-quote, para-ragged-left, para-ragged-right, sans-para-center, sans-para-flush, sans-para-indent, \n",
      "Cantidad de archivos en <data/book1>: 5\n",
      "Parrafos validos? True\n",
      "Cantidad total de parrafos: 2046\n",
      "\n",
      "\n",
      "Tag html a analizar: <p>\n",
      "Clases html a analizar: para-flush, para-indent\n",
      "Cantidad de archivos en <data/book2>: 18\n",
      "Parrafos validos? True\n",
      "Cantidad total de parrafos: 3941\n",
      "\n",
      "\n",
      "Tag html a analizar: <div>\n",
      "Clases html a analizar: tx, tx1\n",
      "Cantidad de archivos en <data/book3>: 8\n",
      "Parrafos validos? False\n",
      "Cantidad total de parrafos: 4554\n",
      "\n",
      "\n",
      "Tag html a analizar: <p>\n",
      "Clases html a analizar: begin, para-indent, para-flush, para-ragged-left\n",
      "Cantidad de archivos en <data/book4>: 33\n",
      "Parrafos validos? True\n",
      "Cantidad total de parrafos: 6646\n",
      "\n",
      "\n",
      "Tag html a analizar: <p>\n",
      "Clases html a analizar: block, blockb, blocki, indent, indent1, indent1b, indenta, indentab, indentb, indenti, noindent, noindenta, noindentb\n",
      "Cantidad de archivos en <data/book5>: 26\n",
      "Parrafos validos? True\n",
      "Cantidad total de parrafos: 6556\n",
      "\n",
      "\n",
      "Tag html a analizar: <p>\n",
      "Clases html a analizar: block, blocki, blockt, blockti, indent, indenta, indentb, noindent, noindent1, noindenta, noindentb, poem, poem1, poema, right, right1\n",
      "Cantidad de archivos en <data/book6>: 15\n",
      "Parrafos validos? True\n",
      "Cantidad total de parrafos: 3727\n",
      "\n",
      "\n",
      "Tag html a analizar: <p>\n",
      "Clases html a analizar: indent, indentb, indentb1, noindent, noindentc, noindentn, noindentn1\n",
      "Cantidad de archivos en <data/book7>: 36\n",
      "Parrafos validos? True\n",
      "Cantidad total de parrafos: 6375\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l_par = []\n",
    "for book, html_tag, html_clases in zip(listdir, list_tag, list_class):\n",
    "    print('Tag html a analizar: <' + html_tag + '>')\n",
    "    print('Clases html a analizar: ' + ', '.join(html_clases))\n",
    "    parrafos = leer_libro(book, html_tag, html_clases)\n",
    "    l_par.append(parrafos)\n",
    "    print\n",
    "    print    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from corenlp import *\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = StanfordNERTagger('../stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "\t\t\t\t\t   '../stanford-ner/stanford-ner.jar',\n",
    "\t\t\t\t\t   encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jsonrpc import ServerProxy, JsonRpc20, TransportTcpIp\n",
    "\n",
    "class StanfordNLP:\n",
    "    def __init__ (self):\n",
    "        self.server = ServerProxy(JsonRpc20(),\n",
    "                                 TransportTcpIp(addr=(\"127.0.0.1\", 8080)))\n",
    "    def parse (self, text):\n",
    "        return json.loads(self.server.parse(text))\n",
    "    \n",
    "nlp = StanfordNLP()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alias = {\n",
    "    'Roland': ['the gunslinger'],\n",
    "    'Eddie': ['a drog addict', 'the prisioner'],\n",
    "    'Jake': ['Bama', 'the boy'],\n",
    "    'Oy': ['billy-bumbler', 'oy'],\n",
    "    'Cuthbert': ['Bert'], \n",
    "    'John':  ['the good man'],\n",
    "    'Randal': ['the man in black', 'the ageless stranger', 'the walking dude',\\\n",
    "              'walter o\\'dim','walter o`dim', 'marten broadcloak', 'richard faninn', \\\n",
    "             'rudin filaro', 'legion', 'covenant man', 'walter padick', 'son of sam'],\n",
    "    'Aballah': ['the crimson king', 'the ultimate evil', 'ram aballah'],\n",
    "    'Jack': ['the pusher'],\n",
    "    'Blaine': ['blaine the mono'],\n",
    "    'Rhea': ['rhea of the cöos'],\n",
    "    'Andrew': ['the tick-tock man']\n",
    "}\n",
    "\n",
    "global_entidades = set(['Roland', 'Eddie', 'Jake', 'Oy',\n",
    "              'Cuthbert', 'John', 'Randal', 'Aballah',\n",
    "              'Jack', 'Blaine', 'Rhea', 'Andrew'])\n",
    "\n",
    "coincidencias = {\n",
    "    'Roland': 0, 'Eddie': 0, 'Jake': 0, 'Oy': 0, 'Cuthbert': 0,\n",
    "    'John': 0, 'Randal': 0, 'Aballah': 0, 'Jack': 0,\n",
    "    'Blaine': 0, 'Rhea': 0, 'Andrew': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def revisar_parrafo_tok(p, alias):\n",
    "    p_tok = word_tokenize(p.decode('utf-8') , language='english')\n",
    "    for word in p:\n",
    "        if (alias == word):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def cambiar_alias(parrafo, alias, coincidencias):\n",
    "    new_p = parrafo\n",
    "    for entidad, alias in alias.iteritems():\n",
    "        for to_replace in alias:\n",
    "            #to low\n",
    "            p_low = new_p.lower()\n",
    "            #buscar substr\n",
    "            coincide = True\n",
    "            if (len(to_replace.split(' ')) == 1):\n",
    "                coincide = revisar_parrafo_tok(p_low, to_replace)\n",
    "            ind_st = p_low.find(to_replace)\n",
    "            if (ind_st < 0 or not coincide): continue\n",
    "            ind_lt = ind_st + len(to_replace)\n",
    "            #remplazar por entidad\n",
    "            new_p = new_p[:ind_st] + entidad + new_p[ind_lt:]\n",
    "            #aumentar contador de apariencias\n",
    "            coincidencias[entidad] += 1\n",
    "    return new_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_book_par = []\n",
    "\n",
    "for book in l_par:\n",
    "    new_b = [cambiar_alias(p, alias, coincidencias) for p in book]\n",
    "    new_book_par.append(new_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatenar_parrafos_cortos(book, min_sent):\n",
    "    new_book = []\n",
    "    book_len = len(book)\n",
    "    bef_short = False\n",
    "    for p in book:\n",
    "        if bef_short:\n",
    "            last_char = new_book[-1][-1]\n",
    "            if (last_char == '.' or last_char == '!' or last_char == '?' or last_char=='\"'):\n",
    "                new_book[-1] = new_book[-1] + ' ' + p  \n",
    "            else:\n",
    "                new_book[-1] = new_book[-1] + '. ' + p\n",
    "        else:\n",
    "            new_book.append(p)\n",
    "        bef_short = len(sent_tokenize(p.decode('utf-8'))) <= min_sent\n",
    "    return new_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Minima sentencias en un parrafo\n",
    "min_sent = 3\n",
    "\n",
    "newest_book_par = [concatenar_parrafos_cortos(b, min_sent) for b in new_book_par]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aballah': 127,\n",
       " 'Andrew': 82,\n",
       " 'Blaine': 56,\n",
       " 'Cuthbert': 0,\n",
       " 'Eddie': 0,\n",
       " 'Jack': 8,\n",
       " 'Jake': 740,\n",
       " 'John': 38,\n",
       " 'Oy': 0,\n",
       " 'Randal': 239,\n",
       " 'Rhea': 0,\n",
       " 'Roland': 1653}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coincidencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizar_parrafo(text):\n",
    "    tokenized_sentences = nltk.sent_tokenize(text)\n",
    "    sents_token=[nltk.word_tokenize(sentence) for sentence in tokenized_sentences]\n",
    "    return sents_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unificador_nnp_extractor_verbos(sents_token):\n",
    "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in sents_token]\n",
    "    ner_sentences = [st.tag(sentence) for sentence in sents_token]\n",
    "    paragraph_untokenized=[[] for s in sents_token]\n",
    "    verbs=[[] for s in sents_token]\n",
    "    entities=[{\"PERSON\":[],\"ORGANIZATION\":[],\"LOCATION\":[]} for s in sents_token]\n",
    "    nnp_before = False\n",
    "    for sentenceNer,sentencePos, i in zip(ner_sentences,tagged_sentences,range(len(ner_sentences))):\n",
    "        flag_aux = False\n",
    "        flag_ver = False\n",
    "        flag_final = False\n",
    "        polarity = False\n",
    "        for j,(wordNer,tagNer),(wordPos,tagPos) in zip(range(len(sentenceNer)),sentenceNer,sentencePos):\n",
    "            if j > 0 and tagNer != 'O' :\n",
    "                if nnp_before:\n",
    "                    paragraph_untokenized[i][-1] +=  wordNer\n",
    "                    entities[i][tagNer][-1] += wordNer\n",
    "                else:\n",
    "                    paragraph_untokenized[i].append(wordNer)\n",
    "                    entities[i][tagNer].append(wordNer)\n",
    "                nnp_before=True\n",
    "            else:\n",
    "                nnp_before = False\n",
    "                paragraph_untokenized[i].append(wordNer)\n",
    "                \n",
    "            polarity = flag_final\n",
    "            if flag_final:\n",
    "                verbs[i].pop()\n",
    "            \n",
    "            flag_aux = ('RB' == tagPos) and (wordPos == \"n't\" or wordPos == \"not\")\n",
    "                \n",
    "            flag_final = flag_aux and flag_ver\n",
    "                \n",
    "            if 'VB' in tagPos:\n",
    "                verbs[i].append((wordPos, polarity))\n",
    "                flag_ver = True\n",
    "            else:\n",
    "                flag_ver = False\n",
    "                \n",
    "    #Llenar entidades globales\n",
    "    for e in entities:\n",
    "        for o in e['PERSON']:\n",
    "            global_entidades.add(o)\n",
    "        for o in e['ORGANIZATION']:\n",
    "            global_entidades.add(o)\n",
    "        for o in e['LOCATION']:\n",
    "            global_entidades.add(o);\n",
    "    \n",
    "    return verbs,entities\n",
    "    \n",
    "    \n",
    "    #return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extraer_dato_parrafo(parrafo):\n",
    "    token_par=tokenizar_parrafo(parrafo)\n",
    "    verbos,entidades = unificador_nnp_extractor_verbos(token_par)\n",
    "    new_p = []\n",
    "    for v, e in zip(verbos, entidades):\n",
    "        new_p.append({\n",
    "            'verbos': v,\n",
    "            'entidades': e\n",
    "        })\n",
    "    return new_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def convertir_libros_verbos_entidades(book, i):\n",
    "    new_book = []\n",
    "    start_book = time.time()\n",
    "    print ('Libro parte: ' + str(i) + '\\n')\n",
    "    for j, p in enumerate(book):\n",
    "        start_p = time.time()\n",
    "        print ('Parrafo'+ str(i) + ' : ' + str(j) + '/' + str(len(book)) + '\\n')\n",
    "        new_book.append(extraer_dato_parrafo(p))\n",
    "        end_p = time.time()\n",
    "        print ('Tiempo parrafo' + str(i) +  ': ' + str(end_p - start_p) + '\\n')\n",
    "    end_book = time.time()\n",
    "    print ('Tiempo libro ' + str(i) + ': ' + str(end_book - start_book) + '\\n')\n",
    "    return new_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libro parte: 1\n",
      "\n",
      "Parrafo1 : 0/100\n",
      "\n",
      "Tiempo parrafo1: 29.429721117\n",
      "\n",
      "Parrafo1 : 1/100\n",
      "\n",
      "Tiempo parrafo1: 56.2941679955\n",
      "\n",
      "Parrafo1 : 2/100\n",
      "\n",
      "Tiempo parrafo1: 39.6737189293\n",
      "\n",
      "Parrafo1 : 3/100\n",
      "\n",
      "Tiempo parrafo1: 34.4788868427\n",
      "\n",
      "Parrafo1 : 4/100\n",
      "\n",
      "Tiempo parrafo1: 26.5298099518\n",
      "\n",
      "Parrafo1 : 5/100\n",
      "\n",
      "Tiempo parrafo1: 26.1231789589\n",
      "\n",
      "Parrafo1 : 6/100\n",
      "\n",
      "Tiempo parrafo1: 57.5013360977\n",
      "\n",
      "Parrafo1 : 7/100\n",
      "\n",
      "Tiempo parrafo1: 17.749943018\n",
      "\n",
      "Parrafo1 : 8/100\n",
      "\n",
      "Tiempo parrafo1: 27.9246108532\n",
      "\n",
      "Parrafo1 : 9/100\n",
      "\n",
      "Tiempo parrafo1: 18.6976630688\n",
      "\n",
      "Parrafo1 : 10/100\n",
      "\n",
      "Tiempo parrafo1: 15.4789409637\n",
      "\n",
      "Parrafo1 : 11/100\n",
      "\n",
      "Tiempo parrafo1: 33.3895480633\n",
      "\n",
      "Parrafo1 : 12/100\n",
      "\n",
      "Tiempo parrafo1: 81.4949600697\n",
      "\n",
      "Parrafo1 : 13/100\n",
      "\n",
      "Tiempo parrafo1: 37.9494590759\n",
      "\n",
      "Parrafo1 : 14/100\n",
      "\n",
      "Tiempo parrafo1: 22.0994169712\n",
      "\n",
      "Parrafo1 : 15/100\n",
      "\n",
      "Tiempo parrafo1: 30.8176021576\n",
      "\n",
      "Parrafo1 : 16/100\n",
      "\n",
      "Tiempo parrafo1: 36.0715310574\n",
      "\n",
      "Parrafo1 : 17/100\n",
      "\n",
      "Tiempo parrafo1: 45.2569770813\n",
      "\n",
      "Parrafo1 : 18/100\n",
      "\n",
      "Tiempo parrafo1: 90.3143079281\n",
      "\n",
      "Parrafo1 : 19/100\n",
      "\n",
      "Tiempo parrafo1: 79.2068929672\n",
      "\n",
      "Parrafo1 : 20/100\n",
      "\n",
      "Tiempo parrafo1: 59.054680109\n",
      "\n",
      "Parrafo1 : 21/100\n",
      "\n",
      "Tiempo parrafo1: 19.3642439842\n",
      "\n",
      "Parrafo1 : 22/100\n",
      "\n",
      "Tiempo parrafo1: 74.944327116\n",
      "\n",
      "Parrafo1 : 23/100\n",
      "\n",
      "Tiempo parrafo1: 21.6288790703\n",
      "\n",
      "Parrafo1 : 24/100\n",
      "\n",
      "Tiempo parrafo1: 65.4805419445\n",
      "\n",
      "Parrafo1 : 25/100\n",
      "\n",
      "Tiempo parrafo1: 34.2876200676\n",
      "\n",
      "Parrafo1 : 26/100\n",
      "\n",
      "Tiempo parrafo1: 129.058436871\n",
      "\n",
      "Parrafo1 : 27/100\n",
      "\n",
      "Tiempo parrafo1: 27.2353830338\n",
      "\n",
      "Parrafo1 : 28/100\n",
      "\n",
      "Tiempo parrafo1: 105.811104059\n",
      "\n",
      "Parrafo1 : 29/100\n",
      "\n",
      "Tiempo parrafo1: 36.4766891003\n",
      "\n",
      "Parrafo1 : 30/100\n",
      "\n",
      "Tiempo parrafo1: 46.1465220451\n",
      "\n",
      "Parrafo1 : 31/100\n",
      "\n",
      "Tiempo parrafo1: 62.7614059448\n",
      "\n",
      "Parrafo1 : 32/100\n",
      "\n",
      "Tiempo parrafo1: 28.213668108\n",
      "\n",
      "Parrafo1 : 33/100\n",
      "\n",
      "Tiempo parrafo1: 58.6434309483\n",
      "\n",
      "Parrafo1 : 34/100\n",
      "\n",
      "Tiempo parrafo1: 114.643765926\n",
      "\n",
      "Parrafo1 : 35/100\n",
      "\n",
      "Tiempo parrafo1: 25.8683137894\n",
      "\n",
      "Parrafo1 : 36/100\n",
      "\n",
      "Tiempo parrafo1: 36.8229050636\n",
      "\n",
      "Parrafo1 : 37/100\n",
      "\n",
      "Tiempo parrafo1: 39.6616580486\n",
      "\n",
      "Parrafo1 : 38/100\n",
      "\n",
      "Tiempo parrafo1: 46.55220294\n",
      "\n",
      "Parrafo1 : 39/100\n",
      "\n",
      "Tiempo parrafo1: 88.6591219902\n",
      "\n",
      "Parrafo1 : 40/100\n",
      "\n",
      "Tiempo parrafo1: 115.603559017\n",
      "\n",
      "Parrafo1 : 41/100\n",
      "\n",
      "Tiempo parrafo1: 307.170449972\n",
      "\n",
      "Parrafo1 : 42/100\n",
      "\n",
      "Tiempo parrafo1: 51.5828220844\n",
      "\n",
      "Parrafo1 : 43/100\n",
      "\n",
      "Tiempo parrafo1: 98.9290890694\n",
      "\n",
      "Parrafo1 : 44/100\n",
      "\n",
      "Tiempo parrafo1: 105.981859922\n",
      "\n",
      "Parrafo1 : 45/100\n",
      "\n",
      "Tiempo parrafo1: 55.8001608849\n",
      "\n",
      "Parrafo1 : 46/100\n",
      "\n",
      "Tiempo parrafo1: 124.057433128\n",
      "\n",
      "Parrafo1 : 47/100\n",
      "\n",
      "Tiempo parrafo1: 112.261040926\n",
      "\n",
      "Parrafo1 : 48/100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#parte 1\n",
    "book1 = newest_book_par[0][0:100]\n",
    "new_book_1 = convertir_libros_verbos_entidades(book1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#para parrafos que contengan m'as de una oraci'on\n",
    "def reemplazar_referencias_parrafo(text, nlp):\n",
    "    print 'Original text:'\n",
    "    print(text)\n",
    "    #nlp = StanfordCoreNLP()\n",
    "    result = nlp.parse(text)\n",
    "    tokenized_sentences = nltk.sent_tokenize(text)\n",
    "    if (len(tokenized_sentences) == 1) :\n",
    "        print \" >  Solo hay una oracion, no existen relaciones.\"\n",
    "        return\n",
    "    \n",
    "    tokenized_in_words=[nltk.word_tokenize(sentence) for sentence in tokenized_sentences]\n",
    "\n",
    "    for block_to_replace in result[\"coref\"]:\n",
    "        sentence_index=block_to_replace[0][1][1]\n",
    "        word_index=block_to_replace[0][1][2]\n",
    "        replace_sent=block_to_replace[0][1][0]\n",
    "        word_to_replace=tokenized_in_words[sentence_index][word_index]\n",
    "\n",
    "        if not word_to_replace in personajes:\n",
    "            continue\n",
    "\n",
    "\n",
    "        print 'Word_to_replace: ',word_to_replace\n",
    "\n",
    "        for i,lines_to_replace in enumerate(block_to_replace):\n",
    "            print \"Converting sentence number \",i\n",
    "\n",
    "            ix_sent=lines_to_replace[0][1]\n",
    "            sent_to_replace=lines_to_replace[0][0]\n",
    "            tokenized_sentences[ix_sent]=tokenized_sentences[ix_sent].replace(sent_to_replace,word_to_replace)\n",
    "\n",
    "        tokenized_sentences[sentence_index]=tokenized_sentences[sentence_index].replace(replace_sent,word_to_replace)\n",
    "        print\n",
    "\n",
    "    final_text=' '.join(tokenized_sentences)\n",
    "    return final_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print reemplazar_referencias_parrafo(newest_book_par[0][10] , nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def senti_verb(verb):\n",
    "    list_meanings=swn.senti_synsets(verb[0])\n",
    "    pos_value=0\n",
    "    neg_value=0\n",
    "    n_vals=len(list_meanings)\n",
    "    meaning_counter=0\n",
    "    for meaning in list_meanings:\n",
    "        lit_string=str(meaning)\n",
    "        if not '.v.' in lit_string:\n",
    "            continue\n",
    "        pos_value+=meaning.pos_score()\n",
    "        neg_value+=meaning.neg_score()\n",
    "        meaning_counter+=1\n",
    "        if meaning_counter == 3: break\n",
    "    if n_vals>0:\n",
    "        pos_value/=n_vals\n",
    "        neg_value/=n_vals\n",
    "    if (verb[1]):\n",
    "        return neg_value,pos_value\n",
    "    else:\n",
    "        return pos_value,neg_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print verbos[0][0]\n",
    "print senti_verb(verbos[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
