{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join, isdir\n",
    "import json\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from corenlp import *\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Models: 5/5                                                            \n"
     ]
    }
   ],
   "source": [
    "from jsonrpc import ServerProxy, JsonRpc20, TransportTcpIp\n",
    "\n",
    "class StanfordNLP:\n",
    "    def __init__ (self):\n",
    "        self.server = ServerProxy(JsonRpc20(),\n",
    "                                 TransportTcpIp(addr=(\"127.0.0.1\", 8080)))\n",
    "    def parse (self, text):\n",
    "        return json.loads(self.server.parse(text))\n",
    "    \n",
    "nlp = StanfordCoreNLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de libros: 4\n"
     ]
    }
   ],
   "source": [
    "dir_path = 'output'\n",
    "#os.listdir(dir_path)\n",
    "listdir = [dir_path + '/' + d for d in os.listdir(dir_path) if isdir(join(dir_path, d))]\n",
    "listdir.sort()\n",
    "\n",
    "print ('Cantidad de libros: ' + str(len(listdir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_obj():\n",
    "    return {\n",
    "        'verbos': [],\n",
    "        'entidades': {\n",
    "            'PERSON': set([]),\n",
    "            'LOCATION': set([]),\n",
    "            'ORGANIZATION': set([])\n",
    "        }\n",
    "    }\n",
    "\n",
    "def my_unidecode(partial):\n",
    "    new_partial = []\n",
    "    for p in partial:\n",
    "        new_p = []\n",
    "        for s in p:\n",
    "            obj = new_obj()\n",
    "            for v in s['verbos']:\n",
    "                obj['verbos'].append([unidecode(v[0]),v[1]])\n",
    "            for o in s['entidades']['PERSON']:\n",
    "                obj['entidades']['PERSON'].add(unidecode(o))\n",
    "            for o in s['entidades']['LOCATION']:\n",
    "                obj['entidades']['LOCATION'].add(unidecode(o))\n",
    "            for o in s['entidades']['ORGANIZATION']:\n",
    "                obj['entidades']['ORGANIZATION'].add(unidecode(o))\n",
    "            new_p.append(obj)\n",
    "        new_partial.append(new_p)\n",
    "    return new_partial             \n",
    "\n",
    "def read_book(book_path):\n",
    "    list_files = [book_path + '/' + f for f in os.listdir(book_path) if isfile(join(book_path, f))]\n",
    "    list_files.sort()\n",
    "    book = []\n",
    "    for path_file in list_files:\n",
    "        with open(path_file) as f:\n",
    "            partial = json.load(f)\n",
    "            partial_decoded = my_unidecode(partial)\n",
    "            book.append(partial_decoded)\n",
    "    out = [p for book_partial in book for p in book_partial]\n",
    "    return out\n",
    "\n",
    "def read_test():\n",
    "    list_files = ['test' + str(i) + '.json' for i in range(19)]\n",
    "    book = []\n",
    "    for path_file in list_files:\n",
    "        with open(path_file) as f:\n",
    "            partial = json.load(f)\n",
    "            book.append(partial)\n",
    "    out = [p for book_partial in book for p in book_partial]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leer_book0():    \n",
    "    with open('book0.json') as f:\n",
    "        book0 = json.load(f)\n",
    "        book0_text = [unidecode(p) for p in book0]\n",
    "    book0_json = read_book(listdir[0])\n",
    "    return book0_text, book0_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book0 = leer_book0()\n",
    "all_ent = {\n",
    "    'PERSON': set(['Roland', 'Eddie', 'Jake', 'Oy',\n",
    "              'Cuthbert', 'John', 'Randal', 'Aballah',\n",
    "              'Jack', 'Blaine', 'Rhea', 'Andrew']),\n",
    "    'LOCATION': set([]),\n",
    "    'ORGANIZATION': set([])\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generar_entidades(book):\n",
    "    for p in book:\n",
    "        for s in p:\n",
    "            for o in s['entidades']['PERSON']:\n",
    "                all_ent['PERSON'].add(o)\n",
    "            for o in s['entidades']['LOCATION']:\n",
    "                all_ent['LOCATION'].add(o)\n",
    "            for o in s['entidades']['ORGANIZATION']:\n",
    "                all_ent['ORGANIZATION'].add(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generar_entidades(book0[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOCATION': {'Antichrist',\n",
       "  'Charyou',\n",
       "  'Commala',\n",
       "  'Cort',\n",
       "  'Cuthbert',\n",
       "  'Damascus',\n",
       "  'Earth',\n",
       "  'End-World',\n",
       "  'Garlan',\n",
       "  'Israel',\n",
       "  'JerichoHill',\n",
       "  'Marten',\n",
       "  'Mejis',\n",
       "  'MohaineDesert',\n",
       "  'Mount',\n",
       "  'NewCanaan',\n",
       "  'NewEarth',\n",
       "  'NewJerseyTurnpike',\n",
       "  'Nort',\n",
       "  'NorthCentralPositronics',\n",
       "  'Phoenician',\n",
       "  'Pricetown',\n",
       "  'Randal',\n",
       "  'Roland',\n",
       "  'SouthIslands',\n",
       "  'Spain',\n",
       "  'Taunton',\n",
       "  'TauntonRoad',\n",
       "  'Tull',\n",
       "  'moon'},\n",
       " 'ORGANIZATION': {'AMOCO',\n",
       "  'ClayBlaisdellWestern',\n",
       "  'Coca-Cola',\n",
       "  'DarkTower',\n",
       "  'EastWing',\n",
       "  'Ford',\n",
       "  'Gilead',\n",
       "  'HorseheadNebula',\n",
       "  'PiperSchool',\n",
       "  \"Sheb's\",\n",
       "  'TheNetwork',\n",
       "  'Town',\n",
       "  'West-Town'},\n",
       " 'PERSON': {'Aballah',\n",
       "  'Abednego',\n",
       "  'Ahaz',\n",
       "  'AileenRitter',\n",
       "  'Alain',\n",
       "  'AlainJohns',\n",
       "  'Alan',\n",
       "  'AlgulSiento',\n",
       "  'Alice',\n",
       "  'Allie',\n",
       "  'AmyFeldon',\n",
       "  'Andrew',\n",
       "  'Arthur',\n",
       "  'ArthurEld',\n",
       "  'AuntMill',\n",
       "  'Bathsheba',\n",
       "  'Bert',\n",
       "  'Blaine',\n",
       "  'Bloomie',\n",
       "  'Brown',\n",
       "  'Castner',\n",
       "  'Chambers',\n",
       "  'Charles',\n",
       "  'CharlesofCharles',\n",
       "  'Charlie',\n",
       "  'CoffinHunter',\n",
       "  'Cook',\n",
       "  'Cort',\n",
       "  'Cuthbert',\n",
       "  'CuthbertAllgood',\n",
       "  'Daniel',\n",
       "  'Davey',\n",
       "  'David',\n",
       "  'Delgado',\n",
       "  'Deschain',\n",
       "  'Dinh',\n",
       "  'Doc',\n",
       "  'Eddie',\n",
       "  'EldredJonas',\n",
       "  'ElmerChambers',\n",
       "  'Farson',\n",
       "  'GabrielleVerriss',\n",
       "  'GretaShaw',\n",
       "  'Hax',\n",
       "  'Hendrickson',\n",
       "  'Hill',\n",
       "  'Horn',\n",
       "  'Isaac',\n",
       "  'Jack',\n",
       "  'Jake',\n",
       "  'JakeChambers',\n",
       "  'Jakedan-dinh',\n",
       "  'Jakes',\n",
       "  'Jamie',\n",
       "  'JamieDeCurry',\n",
       "  'Jeremiah',\n",
       "  'Jesus',\n",
       "  'Jezebel',\n",
       "  'John',\n",
       "  'JohnChambers',\n",
       "  'Jonas',\n",
       "  'Jonson',\n",
       "  'Joshua',\n",
       "  'Jude',\n",
       "  'Kennerly',\n",
       "  'LaMerk',\n",
       "  'Larchies',\n",
       "  'LeMark',\n",
       "  'Maggie',\n",
       "  'ManJesus',\n",
       "  'Manni',\n",
       "  'Mark',\n",
       "  'Marten',\n",
       "  'Mary',\n",
       "  'Merlin',\n",
       "  'Mill',\n",
       "  'MistuhNorton',\n",
       "  'Moses',\n",
       "  'Mother',\n",
       "  'Mutie',\n",
       "  'Narcissus',\n",
       "  'Nort',\n",
       "  'Oy',\n",
       "  'Pittston',\n",
       "  'Randal',\n",
       "  'Rhea',\n",
       "  'Robeson',\n",
       "  'Roland',\n",
       "  'Samson',\n",
       "  'Shaw',\n",
       "  'Sheb',\n",
       "  'Silva',\n",
       "  'Soobie',\n",
       "  'St.Paul',\n",
       "  'StarWormword',\n",
       "  'Steven',\n",
       "  'StevenDeschain',\n",
       "  'Susan',\n",
       "  'SusanDelgado',\n",
       "  'SylviaPittston',\n",
       "  'Thomas',\n",
       "  'ThomasWhitman',\n",
       "  'Tull',\n",
       "  'Ulysses',\n",
       "  'Vannay',\n",
       "  'Walter',\n",
       "  \"Waltero'Dim\",\n",
       "  'Yar',\n",
       "  'Zachary',\n",
       "  'Zoltan',\n",
       "  'gore',\n",
       "  'kenMerlin'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatenar(ori,cop):\n",
    "    for so, sp in zip(ori,cop):\n",
    "        for o in sp['entidades']['PERSON']:\n",
    "            so['entidades']['PERSON'].add(o);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reemplazar_referencias_parrafo(text, json_):\n",
    "    #print 'Original text:'\n",
    "    #print(text)\n",
    "    #nlp = StanfordCoreNLP()\n",
    "    result = json.loads(nlp.parse(text))\n",
    "    tokenized_sentences = nltk.sent_tokenize(text)\n",
    "    if (len(tokenized_sentences) == 1) :\n",
    "        print \" >  Solo hay una oracion, no existen relaciones.\"\n",
    "        return\n",
    "    \n",
    "    tokenized_in_words=[nltk.word_tokenize(sentence) for sentence in tokenized_sentences]\n",
    "    \n",
    "    copia= [ new_obj() for s in json_]\n",
    "    \n",
    "    #print tokenized_in_words\n",
    "    \n",
    "\n",
    "    \n",
    "    for block_to_replace in result[\"coref\"]:\n",
    "        #sentence_index=block_to_replace[0][1][1]\n",
    "        #word_index=block_to_replace[0][1][2]\n",
    "        #replace_sent=block_to_replace[0][1][0]\n",
    "        \n",
    "        word_to_replace=block_to_replace[0][0][0]\n",
    "\n",
    "        if not word_to_replace in all_ent[\"PERSON\"]:\n",
    "            continue\n",
    "\n",
    "\n",
    "        print ' -- Word_to_replace: ',word_to_replace\n",
    "\n",
    "        for i,lines_to_replace in enumerate(block_to_replace):\n",
    "            \n",
    "            ix_sent=lines_to_replace[0][1]\n",
    "            copia[ix_sent]['entidades']['PERSON'].add(word_to_replace)\n",
    "            #sent_to_replace=lines_to_replace[0][0]\n",
    "            #tokenized_sentences[ix_sent]=tokenized_sentences[ix_sent].replace(sent_to_replace,word_to_replace)\n",
    "\n",
    "        #tokenized_sentences[sentence_index]=tokenized_sentences[sentence_index].replace(replace_sent,word_to_replace)\n",
    "        print\n",
    "\n",
    "    concatenar(json_,copia)\n",
    "    return json_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parrafo  0\n",
      "Parrafo  1\n",
      " -- Word_to_replace:  Roland\n",
      "\n",
      " -- Word_to_replace:  Manni\n",
      "\n",
      "Parrafo  2\n",
      "Parrafo  3\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  4\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  5\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  6\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  7\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  8\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  9\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  10\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  11\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  12\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  13\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  14\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  15\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  16\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  17\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  18\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  19\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  20\n",
      "\n",
      "SKIPPING BECAUSE LIFE\n",
      "\n",
      "Parrafo  21\n",
      "Parrafo  22\n"
     ]
    }
   ],
   "source": [
    "for i,parT, parJ in zip(range(len(book0[0])),book0[0],book0[1]):\n",
    "\n",
    "    print 'Parrafo ',i\n",
    "    #print parT\n",
    "    try:\n",
    "        reemplazar_referencias_parrafo(parT,parJ)\n",
    "    except:\n",
    "        print\n",
    "        print 'SKIPPING BECAUSE LIFE'\n",
    "        print\n",
    "    #break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
